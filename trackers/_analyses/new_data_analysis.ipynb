{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import dateutil.parser\n",
    "from dateutil.tz import gettz\n",
    "import datetime\n",
    "import pytz\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tone_names = ['Sadness', 'Analytical', 'Joy', 'Fear', 'Tentative', 'Anger', 'Confident']\n",
    "tone_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#keylogger DF\n",
    "\n",
    "def roundTime(dt):\n",
    "    dt = dt - datetime.timedelta(minutes=dt.minute, seconds=dt.second, microseconds=dt.microsecond)\n",
    "    return dt\n",
    "\n",
    "with open('../keylogger/logs/log_new.json', 'r') as f:\n",
    "    keyloggerData = json.load(f)\n",
    "\n",
    "tzinfos = { \"EDT\" : gettz(\"America/New_York\") }\n",
    "\n",
    "def extract_keyloggerData(data):\n",
    "    results = []\n",
    "\n",
    "    for d in data:\n",
    "        result = [0]*(len(tone_names) + 7)\n",
    "        try:\n",
    "            result[7] = d['word_count']\n",
    "            result[8] = d['uniqueword_count']\n",
    "            result[9] = d['char_count']\n",
    "            result[10] = d['backspace_count']\n",
    "            result[11] = d['avg_dwelltime']\n",
    "            result[12] = d['avg_flighttime']\n",
    "\n",
    "            tones = d['document_tone']['tones']\n",
    "            for i in range(len(tones)):\n",
    "                score = tones[i]['score']\n",
    "                tone_name = tones[i]['tone_name']\n",
    "                tone_index = tone_names.index(tone_name)\n",
    "                result[tone_index] = score            \n",
    "\n",
    "            time = dateutil.parser.parse(d['time'] + \" EDT\", tzinfos=tzinfos)\n",
    "            result[-1] = time\n",
    "            results.append(tuple(result))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return results\n",
    "\n",
    "keyloggerDF = DataFrame(extract_keyloggerData(keyloggerData),\n",
    "                        columns=[tone_name+\"_score\" for tone_name in tone_names] + ['word_count','uniqueword_count','char_count','backspace_count','avg_dwelltime','avg_flighttime','time'])\n",
    "#keyloggerDF.time = keyloggerDF.time.apply(roundTime)\n",
    "#keyloggerDF.time.apply(roundTime)\n",
    "keyloggerDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#affectiva DF\n",
    "\n",
    "local_tz = gettz('America/New_York')\n",
    "\n",
    "def utc_to_local(utc_dt):\n",
    "    return utc_dt.replace(tzinfo=local_tz)\n",
    "\n",
    "with open('../affectiva/analyses/merged_file.json', 'r') as f:\n",
    "    affectivaData = json.load(f)\n",
    "\n",
    "for x in range(0, len(affectivaData)):\n",
    "    test = datetime.datetime.fromtimestamp((affectivaData[x]['time']/ 1e3))\n",
    "    affectivaData[x]['time'] = utc_to_local(datetime.datetime.fromtimestamp((affectivaData[x]['time']/ 1e3)))\n",
    "\n",
    "affectivaDF = DataFrame(affectivaData)\n",
    "# affectivaDF.time = affectivaDF.time.apply(roundTime)\n",
    "# affectivaDF.time.apply(roundTime)\n",
    "#affectivaDF.emoji = affectivaDF.emoji.apply(lambda x: \",\".join(x))\n",
    "del affectivaDF['emoji']\n",
    "#affectivaDF.emotions = affectivaDF.emotions.apply(lambda x: \",\".join(x))\n",
    "del affectivaDF['emotions']\n",
    "del affectivaDF['max_attention']\n",
    "del affectivaDF['min_attention']\n",
    "del affectivaDF['max_engagement']\n",
    "del affectivaDF['min_engagement']\n",
    "del affectivaDF['max_valence']\n",
    "del affectivaDF['min_valence']\n",
    "\n",
    "affectivaDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mood reporter DF\n",
    "\n",
    "responsesDF = pd.read_csv(\"../reporter/responses.tsv\", sep='\\t', header=0)\n",
    "\n",
    "timeValues = responsesDF.time.values\n",
    "\n",
    "for x in range(0, len(timeValues)):\n",
    "    timeValues[x] = dateutil.parser.parse(timeValues[x] + \" EDT\", tzinfos=tzinfos)\n",
    "\n",
    "# responsesData.time = timeValues\n",
    "#responsesData.time = responsesData.time.apply(roundTime)\n",
    "#responsesData.time.apply(roundTime)\n",
    "\n",
    "activity_names = list(set(list(map(lambda x: x.lower().strip(), reduce(lambda x,y: x+y, [x.split(\",\") for x in list(responsesDF.activity.values)])))))\n",
    "\n",
    "for activity_name in activity_names:\n",
    "    responsesDF[activity_name.replace(\" \", \"_\") + \"_activity\"] = responsesDF.activity.apply(lambda x: activity_name in x.lower())\n",
    "\n",
    "location_names = list(set(list(map(lambda x: x.lower().strip(), reduce(lambda x,y: x+y, [x.split(\",\") for x in list(responsesDF.location.values)])))))\n",
    "\n",
    "def split_locations(locations):\n",
    "    return list(map(lambda x: x.lower().strip(), locations.split(\",\")))\n",
    "\n",
    "for location_name in location_names:\n",
    "    responsesDF[location_name.replace(\" \", \"_\") + \"_location\"] = responsesDF.location.apply(lambda x: location_name in split_locations(x))\n",
    "    \n",
    "del responsesDF['moodNotes']\n",
    "del responsesDF['trigger']\n",
    "del responsesDF['activity']\n",
    "del responsesDF['location']\n",
    "\n",
    "responsesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#productivity DF\n",
    "\n",
    "with open('../getAPIdata/productivity.json', 'r') as f:\n",
    "    productivityFile = json.load(f)\n",
    "\n",
    "productivityData = productivityFile['rows']\n",
    "\n",
    "final_productivityData = [];\n",
    "\n",
    "for x in range(0, len(productivityData)):\n",
    "    if(productivityData[x][0] > '2018-04-01T90:00:00'):\n",
    "        time = dateutil.parser.parse(productivityData[x][0] + \" EDT\", tzinfos=tzinfos)\n",
    "        prod_score = productivityData[x][4]\n",
    "        final_productivityData.append((time, prod_score))\n",
    "\n",
    "productivityDF = DataFrame(final_productivityData, columns=['time', 'productivity_score'])\n",
    "#productivityDF.time = productivityDF.time.apply(roundTime)\n",
    "#productivityDF.time.apply(roundTime)\n",
    "\n",
    "productivityDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exerciseDF = pd.read_csv(\"../S_Health/com.samsung.health.exercise.201808072208.csv\", header=0)\n",
    "exerciseTimeVals = exerciseDF.create_time.values\n",
    "\n",
    "for x in range(0, len(exerciseTimeVals)):\n",
    "    exerciseTimeVals[x] = dateutil.parser.parse(exerciseTimeVals[x] + \" EDT\", tzinfos=tzinfos)\n",
    "\n",
    "exerciseDF.create_time = exerciseTimeVals\n",
    "exerciseDF = exerciseDF[['create_time', 'distance', 'mean_speed', 'calorie']]\n",
    "exerciseDF.columns = ['time', 'distance', 'mean_speed', 'calorie']\n",
    "exerciseDF.sort_values('time', inplace=True)\n",
    "exerciseDF = exerciseDF.reset_index(drop=True)\n",
    "exerciseDF = exerciseDF[(exerciseDF['time'] > '2018-05-31')]\n",
    "\n",
    "exerciseDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o2SatDF = pd.read_csv(\"../S_Health/com.samsung.health.oxygen_saturation.201808072208.csv\", header=0)\n",
    "o2TimeVals = o2SatDF.create_time.values\n",
    "#o2TimeZoneVals = o2SatDF.time_offset.values\n",
    "\n",
    "for x in range(0, len(o2TimeVals)):\n",
    "    o2TimeVals[x] = dateutil.parser.parse(o2TimeVals[x] + \" EDT\", tzinfos=tzinfos)\n",
    "\n",
    "o2SatDF.create_time = o2TimeVals\n",
    "o2SatDF = o2SatDF[['create_time', 'heart_rate', 'spo2']]\n",
    "o2SatDF.columns = ['time', 'heart_rate', 'spo2']\n",
    "o2SatDF.sort_values('time', inplace=True)\n",
    "o2SatDF = o2SatDF.reset_index(drop=True)\n",
    "o2SatDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tabcounter\n",
    "\n",
    "with open('../getAPIdata/chromeactivity.json', 'r') as f:\n",
    "    tabCounterData = json.load(f)\n",
    "\n",
    "newTabData = []\n",
    "\n",
    "for key in tabCounterData:\n",
    "    time = utc_to_local(datetime.datetime.fromtimestamp((int(key)/ 1e3)))\n",
    "    current_tabCount = tabCounterData[key]['current_tabCount']\n",
    "    current_windowCount = tabCounterData[key]['current_windowCount']\n",
    "    tabs_activated = tabCounterData[key]['tabs_activated']\n",
    "    tabs_created = tabCounterData[key]['tabs_created']\n",
    "    windows_created = tabCounterData[key]['windows_created']\n",
    "\n",
    "    newTabData.append([time, current_tabCount, current_windowCount,tabs_activated,tabs_created,windows_created])\n",
    "\n",
    "tabColumns = ['time', 'current_tabCount', 'current_windowCount','tabs_activated','tabs_created','windows_created']\n",
    "tabCounterDF = DataFrame(newTabData, columns=tabColumns)\n",
    "tabCounterDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qualityTimeDF = pd.read_csv(\"../QualityTime/1533931014602_ENTIRE_HISTORY_1470859008205_1533931008205.csv\", header=0)\n",
    "qtTimeVals = qualityTimeDF.End_Time.values\n",
    "\n",
    "newQTtimeVals = []\n",
    "for x in range(0, len(qtTimeVals)):\n",
    "    time = utc_to_local(datetime.datetime.fromtimestamp((int(qtTimeVals[x])/ 1e3)))\n",
    "    newQTtimeVals.append(time)\n",
    "\n",
    "\n",
    "usageVals = qualityTimeDF.Usage.values\n",
    "newQTusageVals = []\n",
    "\n",
    "for x in range(0, len(usageVals)):\n",
    "    string = usageVals[x]\n",
    "    minSubstring = \"min\"\n",
    "    hrSubstring = \"hr\"\n",
    "    finalSecs = 0\n",
    "    \n",
    "    if hrSubstring in string:\n",
    "        removeHr = string[0:1]\n",
    "        newHr = int(removeHr) * 3600\n",
    "        finalSecs += newHr\n",
    "    elif minSubstring in string:\n",
    "        removeMin = string[-6:-4]\n",
    "        newMin = int(removeMin) * 60\n",
    "        finalSecs += newMin\n",
    "    else:\n",
    "        removeSec = string[0:-4]\n",
    "        newSec = int(removeSec)\n",
    "        finalSecs += newSec\n",
    "        \n",
    "    newQTusageVals.append(finalSecs)\n",
    "\n",
    "    \n",
    "qualityTimeDF.End_Time = newQTtimeVals\n",
    "qualityTimeDF.Usage = newQTusageVals\n",
    "qualityTimeDF = qualityTimeDF[['End_Time', 'Usage']]\n",
    "qualityTimeDF.columns = ['time', 'usage_in_sec']\n",
    "\n",
    "compressed = []\n",
    "current_hour = qualityTimeDF['time'][0].replace(microsecond=0,second=0,minute=0)\n",
    "total_usage = 0\n",
    "unlocks = 0\n",
    "\n",
    "for index, row in qualityTimeDF.iterrows():\n",
    "    if (index == len(qualityTimeDF) -1):\n",
    "        holder = [0] * 3\n",
    "        holder[0] = current_hour\n",
    "        holder[1] = unlocks\n",
    "        holder[2] = total_usage\n",
    "        compressed.append(tuple(holder))\n",
    "    elif (row['time'].replace(microsecond=0,second=0,minute=0) == current_hour):\n",
    "        total_usage += row['usage_in_sec']\n",
    "        unlocks +=1\n",
    "    else:\n",
    "        holder = [0] * 3\n",
    "        holder[0] = current_hour\n",
    "        holder[1] = unlocks\n",
    "        holder[2] = total_usage\n",
    "        compressed.append(tuple(holder))\n",
    "        total_usage = 0\n",
    "        unlocks = 0\n",
    "        current_hour = row['time'].replace(microsecond=0,second=0,minute=0)\n",
    "        total_usage += row['usage_in_sec']\n",
    "        unlocks +=1\n",
    "        \n",
    "\n",
    "newQTDF = DataFrame(compressed, columns = ['time', 'unlocks', 'total_usage_in_secs'])\n",
    "newQTDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#QTDF\n",
    "responsesTimeVals = responsesDF.loc[:,'time']\n",
    "responsesTimeVals = responsesTimeVals.apply(roundTime)\n",
    "responsesTimeValsDF = DataFrame(responsesTimeVals)\n",
    "\n",
    "QTframes = [newresponsesTimeValsDF, newQTDF]\n",
    "QTresult = pd.concat(QTframes)\n",
    "QTresult.sort_values('time', inplace=True)\n",
    "QTresult = QTresult.reset_index(drop=True)\n",
    "QTresult = QTresult.fillna(0)\n",
    "\n",
    "QTresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge all the data\n",
    "\n",
    "mergedData = []\n",
    "\n",
    "all_columns = list(responsesDF.columns) + list(keyloggerDF.columns.drop('time')) + list(affectivaDF.columns.drop('time')) + list(productivityDF.columns.drop('time')) + list(o2SatDF.columns.drop('time')) + list(tabCounterDF.columns.drop('time')) + list(newQTDF.columns.drop('time'))\n",
    "\n",
    "def process_row(row):\n",
    "    current_time = row['time']\n",
    "\n",
    "    output_values = list(row.values)\n",
    "\n",
    "    for other_df in [keyloggerDF, affectivaDF, productivityDF, o2SatDF, tabCounterDF, newQTDF]:\n",
    "        candidates = other_df[(current_time > other_df['time']) &\n",
    "                              (current_time - other_df['time'] < datetime.timedelta(hours=5))]\n",
    "        if candidates.empty:\n",
    "            return None\n",
    "        else:\n",
    "            index_of_max = candidates['time'].argmax()\n",
    "            candidate = candidates.ix[index_of_max].drop('time')\n",
    "            output_values += list(candidate.values)\n",
    "    return output_values\n",
    "\n",
    "for index, row in responsesDF.iterrows():\n",
    "    processed_row = process_row(row)\n",
    "    if processed_row is not None:\n",
    "        mergedData.append(processed_row)\n",
    "\n",
    "mergedDF = DataFrame(mergedData, columns = all_columns)\n",
    "mergedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mergedDF_copy = mergedDF[['time']]\n",
    "mergedDFtime = mergedDF.loc[:,'time']\n",
    "mergedDF_copy = DataFrame(mergedDFtime)\n",
    "\n",
    "\n",
    "mergedDF_copy.time.apply(roundTime)\n",
    "#mergedTimeVals = mergedDF_copy[['time']]\n",
    "\n",
    "newExerciseDF = exerciseDF[(exerciseDF['time'] > '2018-05-18')]\n",
    "newExerciseDF.sort_values('time', inplace=True)\n",
    "newExerciseDF = newExerciseDF.reset_index(drop=True)\n",
    "\n",
    "#exerciseDFTimeVals = newExerciseDF[['time']]\n",
    "\n",
    "#for x in range(0, len(newExerciseDF)):\n",
    "    \n",
    "\n",
    "frames = [mergedDF_copy, newExerciseDF]\n",
    "result = pd.concat(frames)\n",
    "result.sort_values('time', inplace=True)\n",
    "result = result.reset_index(drop=True)\n",
    "result = result.fillna(0)\n",
    "\n",
    "\n",
    "columns = list(mergedDF.columns) + list(result.columns.drop('time'))\n",
    "\n",
    "def process_row(row):\n",
    "    current_time = row['time']\n",
    "\n",
    "    output_values = list(row.values)\n",
    "\n",
    "    #for other_df in result:\n",
    "    candidates = result[(current_time > result['time']) &\n",
    "                          (current_time - result['time'] < datetime.timedelta(hours=2))]\n",
    "    if candidates.empty:\n",
    "#         return None\n",
    "        output_values += [0,0,0]\n",
    "    else:\n",
    "        index_of_max = candidates['time'].argmax()\n",
    "        candidate = candidates.ix[index_of_max].drop('time')\n",
    "        \n",
    "        if(candidate['distance'] > 0):\n",
    "            output_values += list(candidate.values)\n",
    "        else:\n",
    "            index_of_min = candidates['time'].argmin()\n",
    "            candidate = candidates.ix[index_of_min].drop('time')\n",
    "            output_values += list(candidate.values)\n",
    "\n",
    "    return output_values\n",
    "\n",
    "newMergedData = []\n",
    "for index, row in mergedDF.iterrows():\n",
    "    processed_row = process_row(row)\n",
    "    if processed_row is not None:\n",
    "        newMergedData.append(processed_row)\n",
    "\n",
    "newMergedDF = DataFrame(newMergedData, columns = columns)\n",
    "newMergedDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "newMergedDF.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newMergedDF.to_csv(\"mergedDF.csv\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr = newMergedDF.corr().fillna(0)\n",
    "# corr.to_csv(\"correlationmatrix.csv\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "pd.plotting.scatter_matrix(newMergedDF, figsize=(88, 88))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(88,88))\n",
    "# plt.matshow(mergedDF.corr())\n",
    "# plt.xticks(range(len(mergedDF.columns)), mergedDF.columns)\n",
    "# plt.yticks(range(len(mergedDF.columns)), mergedDF.columns)\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "cmap = cmap=sns.diverging_palette(5, 250, as_cmap=True)\n",
    "\n",
    "def magnify():\n",
    "    return [dict(selector=\"th\",\n",
    "                 props=[(\"font-size\", \"7pt\")]),\n",
    "            dict(selector=\"td\",\n",
    "                 props=[('padding', \"0em 0em\")]),\n",
    "            dict(selector=\"th:hover\",\n",
    "                 props=[(\"font-size\", \"12pt\")]),\n",
    "            dict(selector=\"tr:hover td:hover\",\n",
    "                 props=[('max-width', '200px'),\n",
    "                        ('font-size', '12pt')])\n",
    "]\n",
    "\n",
    "corr.style.background_gradient(cmap, axis=1)\\\n",
    "    .set_properties(**{'max-width': '30px', 'min-width': '60px', 'min-height': '60px', 'max-height': '60px', 'font-size': '8pt', 'padding': '1em 1em'})\\\n",
    "    .set_caption(\"Hover to magify\")\\\n",
    "    .set_precision(2)\\\n",
    "#     .set_table_styles(magnify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(30, 25))\n",
    "\n",
    "# ax.set_axis_bgcolor(\"white\")\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(359, 230, s=88, l=74, sep=10, n=9, as_cmap=True)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=0, cbar_kws={\"shrink\": 0.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.choose_diverging_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newMergedDF.corr().mood.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mergedDF.columns\n",
    "featuresList = ['errands_activity', 'resting_activity',\n",
    "       'schoolwork_activity', 'therapy_activity', 'school_work_activity',\n",
    "       'just_woke_up_activity', 'ash_stuff_activity', 'workshop_activity',\n",
    "       'leisure_activity', 'eating_activity', 'email_activity',\n",
    "       'with_k_activity', 'reading_activity', 'commuting_activity',\n",
    "       'chatting_activity', 'with_friends_activity', 'in_class_activity',\n",
    "       'kevin_home_location', 'school_location', 'library_location',\n",
    "       'home_location', 'therapy_location', 'Sadness_score',\n",
    "       'Analytical_score', 'Joy_score', 'Fear_score', 'Tentative_score',\n",
    "       'Anger_score', 'Confident_score', 'blinks', 'max_attention',\n",
    "       'max_engagement', 'max_valence', 'min_attention', 'min_engagement',\n",
    "       'min_valence', 'productivity_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "y = mergedDF[\"mood\"]\n",
    "X = mergedDF[featuresList]\n",
    "myLinearModel = linear_model.LinearRegression()\n",
    "myLinearModel = myLinearModel.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myLinearModel.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "list(zip(featuresList,np.round(model.coef_, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "y = mergedDF[\"mood\"]\n",
    "X = mergedDF[featuresList]\n",
    "decision_tree = tree.DecisionTreeRegressor()\n",
    "decision_tree = decision_tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decision_tree.feature_importances_\n",
    "list(zip(featuresList,np.round(model.feature_importances_, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join(str(x)+\", # \" +y for x,y in zip(X.iloc[0].values,featuresList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakehappyX = [False, # errands_activity\n",
    "True, # resting_activity\n",
    "False, # schoolwork_activity\n",
    "True, # therapy_activity\n",
    "False, # school_work_activity\n",
    "False, # just_woke_up_activity\n",
    "False, # ash_stuff_activity\n",
    "False, # workshop_activity\n",
    "True, # leisure_activity\n",
    "True, # eating_activity\n",
    "False, # email_activity\n",
    "True, # with_k_activity\n",
    "False, # reading_activity\n",
    "False, # commuting_activity\n",
    "True, # chatting_activity\n",
    "False, # with_friends_activity\n",
    "False, # in_class_activity\n",
    "True, # kevin_home_location\n",
    "False, # school_location\n",
    "False, # library_location\n",
    "False, # home_location\n",
    "False, # therapy_location\n",
    "0.0, # Sadness_score\n",
    "0.0, # Analytical_score\n",
    "99.0, # Joy_score\n",
    "0.0, # Fear_score\n",
    "0.0, # Tentative_score\n",
    "0.0, # Anger_score\n",
    "0.0, # Confident_score\n",
    "2, # blinks\n",
    "98.7772064209, # max_attention\n",
    "90.76793289185, # max_engagement\n",
    "90.0, # max_valence\n",
    "95.8359222412, # min_attention\n",
    "90.0814501345158, # min_engagement\n",
    "20.81201314926, # min_valence\n",
    "90.14] # productivity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predict([fakehappyX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakeSadX = [True, # errands_activity\n",
    "False, # resting_activity\n",
    "True, # schoolwork_activity\n",
    "False, # therapy_activity\n",
    "True, # school_work_activity\n",
    "False, # just_woke_up_activity\n",
    "True, # ash_stuff_activity\n",
    "False, # workshop_activity\n",
    "False, # leisure_activity\n",
    "False, # eating_activity\n",
    "False, # email_activity\n",
    "False, # with_k_activity\n",
    "False, # reading_activity\n",
    "True, # commuting_activity\n",
    "False, # chatting_activity\n",
    "False, # with_friends_activity\n",
    "False, # in_class_activity\n",
    "False, # kevin_home_location\n",
    "True, # school_location\n",
    "False, # library_location\n",
    "False, # home_location\n",
    "False, # therapy_location\n",
    "99.0, # Sadness_score\n",
    "0.0, # Analytical_score\n",
    "0.0, # Joy_score\n",
    "70.0, # Fear_score\n",
    "40.0, # Tentative_score\n",
    "60.0, # Anger_score\n",
    "0.0, # Confident_score\n",
    "20, # blinks\n",
    "70.7772064209, # max_attention\n",
    "20.76793289185, # max_engagement\n",
    "-40.0, # max_valence\n",
    "65.8359222412, # min_attention\n",
    "10.0814501345158, # min_engagement\n",
    "-90.81201314926, # min_valence\n",
    "40.14] # productivity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predict([fakeSadX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myLinearModel.predict([fakehappyX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7623eb3dbd95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#set aside 20% of the data to use as test data; the rest will be used as training date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[0;32m----> 5\u001b[0;31m     X, y, test_size=0.20, random_state=42)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#train a linear model on the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#set aside 20% of the data to use as test data; the rest will be used as training date\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "#train a linear model on the training data\n",
    "myLinearModel = linear_model.LinearRegression()\n",
    "myLinearModel = myLinearModel.fit(X_train, y_train)\n",
    "\n",
    "#predict on test data and print the results\n",
    "y_pred_lm = myLinearModel.predict(X_test)\n",
    "y_pred_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate the mean absolute error\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mean_absolute_error(y_test, y_pred_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train a decision tree on the training data\n",
    "decision_tree = tree.DecisionTreeRegressor()\n",
    "decision_tree = decision_tree.fit(X_train, y_train)\n",
    "\n",
    "#predict on test data and print the results\n",
    "y_pred_dt = decision_tree.predict(X_test)\n",
    "y_pred_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate the decision tree's mean absolute error\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mean_absolute_error(y_test, y_pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train a random forest on the training data\n",
    "from sklearn import ensemble\n",
    "random_forest = ensemble.RandomForestRegressor()\n",
    "random_forest = random_forest.fit(X_train, y_train)\n",
    "\n",
    "#predict on the test data and print the results\n",
    "y_pred_rf = random_forest.predict(X_test)\n",
    "y_pred_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate the random forest's error\n",
    "mean_absolute_error(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Test values, Random Forest Predictions, Decision Tree Predictions, Linear Model Predictions\")\n",
    "list(zip(y_test, y_pred_rf, y_pred_dt, y_pred_lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictionsDF = DataFrame(list(zip(y_test, y_pred_rf, y_pred_dt, y_pred_lm)), columns=[\"Test values\", \"Random Forest Predictions\", \"Decision Tree Predictions\", \"Linear Model Predictions\"])\n",
    "predictionsDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
