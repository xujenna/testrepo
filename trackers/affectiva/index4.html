<html><head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta name="robots" content="noindex, nofollow">
  <meta name="googlebot" content="noindex, nofollow">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <script type="text/javascript" src="//code.jquery.com/jquery-3.1.0.js"></script><style></style>

    <!-- <link rel="stylesheet" type="text/css" href="/css/result-light.css"> -->
      <script type="text/javascript" src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
      <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css">
      <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
      <!-- <script type="text/javascript" src="https://download.affectiva.com/js/3.2/affdex.js"></script> -->
      <script src="https://download.affectiva.com/js/3.2.1/affdex.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.6.0/p5.js"></script>
      <script src="firebase.js"></script>

  <style type="text/css">

  </style>

  <title>Emotion from Camera Sample App by affectiva</title>


</head>

<body>

  <div class="container-fluid">
    <div class="row">
      <div class="col-md-8" id="affdex_elements" style="width:680px;height:480px;"></div>
      <div class="col-md-4">
        <div style="height:25em;">
          <strong>EMOTION TRACKING RESULTS</strong>
          <div id="results" style="word-wrap:break-word;"></div>
        </div>
        <div>
          <strong>DETECTOR LOG MSGS</strong>
        </div>
        <div id="logs"></div>
      </div>
    </div>
    <div>
      <!-- <button id="start" onclick="onStart()">Start</button>
      <button id="stop" onclick="onStop()">Stop</button>
      <button id="reset" onclick="onReset()">Reset</button> -->
      <div id ="container"></div>
<!--       <h3>Affectiva JS SDK CameraDetector to track different emotions.</h3>
 -->      <!-- <p>
        <strong>Instructions</strong>
        <br>
        Press the start button to start the detector.
        <br> When a face is detected, the probabilities of the different emotions are written to the DOM.
        <br> Press the stop button to end the detector.
      </p> -->
    </div>
  </div>







<script type="text/javascript">//<![CDATA[

      // SDK Needs to create video and canvas nodes in the DOM in order to function
      // Here we are adding those nodes a predefined div.
      var divRoot = $("#affdex_elements")[0];
      var width = 640;
      var height = 480;
      var faceMode = affdex.FaceDetectorMode.LARGE_FACES;
      //Construct a CameraDetector and specify the image width / height and face detector mode.
      var detector = new affdex.CameraDetector(divRoot, width, height, faceMode);
      // var data =[];
      // data.push("{");

      // var a       = document.createElement('a');
      // // a.href      = 'data:' + data;
      // a.download  = 'data.json';
      // a.innerHTML = 'download .json file of json';

      // document.getElementById('container').appendChild(a);

      //Enable detection of all Expressions, Emotions and Emojis classifiers.
      detector.detectAllEmotions();
      detector.detectAllExpressions();
      detector.detectAllEmojis();
      detector.detectAllAppearance();


      var newobject = {};
      var expressionsobj = {};
      var emotionsobj = [];
      var time = Date.now();
      newobject["time"] = time;
      var emojiArray = []
      var valence = [];
      var engagement = [];
      var attention = [];
      var counter = 0;
      var blinks = 0;

      //Add a callback to notify when the detector is initialized and ready for runing.
      detector.addEventListener("onInitializeSuccess", function() {
        log('#logs', "The detector reports initialized");
        //Display canvas instead of video feed because we want to draw the feature points on it
        $("#face_video_canvas").css("display", "block");
        $("#face_video").css("display", "none");
      });

      function log(node_name, msg) {
        $(node_name).append("<span>" + msg + "</span><br />")
      }

      //function executes when Start button is pushed.


      //function executes when the Stop button is pushed.


      function sleep(ms){
        return new Promise(resolve => setTimeout(resolve,ms));
      }

      //Add a callback to notify when camera access is allowed
      detector.addEventListener("onWebcamConnectSuccess", function() {
        log('#logs', "Webcam access allowed");
      });

      //Add a callback to notify when camera access is denied
      detector.addEventListener("onWebcamConnectFailure", function() {
        log('#logs', "webcam denied");
        console.log("Webcam access denied");
      });

      //Add a callback to notify when detector is stopped
      detector.addEventListener("onStopSuccess", function() {
        log('#logs', "The detector reports stopped");
        $("#results").html("");
      });

      //Add a callback to receive the results from processing an image.
      //The faces object contains the list of the faces detected in an image.
      //Faces object contains probabilities for all the different expressions, emotions and appearance metrics
      detector.addEventListener("onImageResultsSuccess", function(faces, image, timestamp) {
        $('#results').html("");


      // var newdata  = "text/json;charset=utf-8," + encodeURIComponent(JSON.stringify(faces[0].expressions));

      // console.log("faces[0].expressions.eyeClosure", faces[0].expressions.eyeClosure);



      var expressions = faces[0].expressions;
      var emotions = faces[0].emotions;


      var bigEvent = false;

      // newobject["attention"] = faces[0].expressions["attention"];


      for (var key in expressions) {
        if (expressions[key] > 95) {
          if(key == "eyeClosure"){
            blinks +=1;
          }
          // expressionsobj[key] = expressions[key];
          // data.push(newobject);
          bigEvent = true;
        }
      }

      for (var key in emotions) {
        if (emotions[key] > 95){
          console.log(key)
          emotionsobj.push(key);
          bigEvent = true;
          // data.push(newobject);
        }
      }

      if (bigEvent) {
        var emoji = faces[0].emojis["dominantEmoji"];
        console.log(emoji)
        emojiArray.push(emoji)
        valence.push(faces[0].emotions["valence"]);
        engagement.push(faces[0].emotions["engagement"]);
        attention.push(faces[0].expressions["attention"]);
        counter +=1;


        // console.log("data",data);
        // a.href      = 'data:' + data;


        bigEvent = false;

      }





      // if (faces[0].expressions.eyeClosure > 99.7 || ) {

      //    data.push(JSON.stringify(faces[0].expressions));
      // // console.log("data", data);
      //     a.href      = 'data:' + data;


      // }

      // console.log("data", data);


        log('#results', "Timestamp: " + timestamp.toFixed(2));
        log('#results', "Number of faces found: " + faces.length);
        if (faces.length > 0) {
          log('#results', "Appearance: " + JSON.stringify(faces[0].appearance));
          log('#results', "Emotions: " + JSON.stringify(faces[0].emotions, function(key, val) {
            return val.toFixed ? Number(val.toFixed(0)) : val;
          }));
          log('#results', "Expressions: " + JSON.stringify(faces[0].expressions, function(key, val) {
            return val.toFixed ? Number(val.toFixed(0)) : val;
          }));
          log('#results', "Emoji: " + faces[0].emojis.dominantEmoji);
          drawFeaturePoints(image, faces[0].featurePoints);
        }
      });

      //Draw the detected facial feature points on the image
      function drawFeaturePoints(img, featurePoints) {
        var contxt = $('#face_video_canvas')[0].getContext('2d');

        var hRatio = contxt.canvas.width / img.width;
        var vRatio = contxt.canvas.height / img.height;
        var ratio = Math.min(hRatio, vRatio);

        contxt.strokeStyle = "#FFFFFF";
        for (var id in featurePoints) {
          contxt.beginPath();
          contxt.arc(featurePoints[id].x,
            featurePoints[id].y, 2, 0, 2 * Math.PI);
          contxt.stroke();

        }
      }

//]]>
// var status = "True";
//
// while(status = "True") {


var config = {
  apiKey: "AIzaSyAhz2WxFzttcO6oZGjDv_8pmKnGLbvvrbs",
  authDomain: "faceanalyzer-73537.firebaseapp.com",
  databaseURL: "https://faceanalyzer-73537.firebaseio.com",
  projectId: "faceanalyzer-73537",
  storageBucket: "",
  messagingSenderId: "621578906992"
};

firebase.initializeApp(config);

  var userID = "xujenna"
  var database = firebase.database();
  var key = "/users/" + userID + "/metrics/" + time;

  detector.start();

  console.log("started");

  setTimeout(function() {
    console.log("stopped");
  detector.removeEventListener();
  detector.stop();
    // a.click();
      emotionsobj = new Set(emotionsobj);
      emojiArray = new Set(emojiArray)

      const reducer = (accumulator, currentValue) => accumulator + currentValue;
      
      if(blinks > 40){
        blinks = Math.sqrt(blinks)
      }

      newobject["time"] = Date.now();
      newobject["emoji"] = Array.from(emojiArray);
      newobject["max_valence"] = Math.max(...valence);
      newobject["min_valence"] = Math.min(...valence);
      newobject["avg_valence"] = valence.reduce(reducer) / valence.length;
      newobject["max_engagement"] = Math.max(...engagement);
      newobject["min_engagement"] = Math.min(...engagement);
      newobject["avg_engagement"] = engagement.reduce(reducer) / engagement.length;
      newobject["max_attention"] = Math.max(...attention);
      newobject["min_attention"] = Math.min(...attention);
      newobject["avg_attention"] = attention.reduce(reducer) / attention.length;
      newobject["blinks"] = blinks;
      newobject["emotions"] = Array.from(emotionsobj);

    database.ref(key).set(newobject);
}, 60000);


// }
</script>

  <script>
  // tell the embed parent frame the height of the content
  if (window.parent && window.parent.parent){
    window.parent.parent.postMessage(["resultsFrame", {
      height: document.body.getBoundingClientRect().height,
      slug: "opyh5e8d"
    }], "*")
  }
</script>





</body></html>
